{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e793aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb06080",
   "metadata": {},
   "source": [
    "# class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #input to spread 2 channels out to 3\n",
    "        self.input = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=1)\n",
    "\n",
    "        #base vision model we're wrapping around with output layer removed\n",
    "        base_model = models.resnet34(pretrained=True)\n",
    "        self.core = nn.Sequential(*list(base_model.children())[:-1])\n",
    "\n",
    "        #custom output layer, with dimension dynamically calculated\n",
    "        dummy = torch.randn(1, 3, 600, 600)\n",
    "        dummy_out = self.core(dummy)\n",
    "        flattened_dim = dummy_out.view(1, -1).shape[1]\n",
    "        self.output = nn.Linear(in_features=flattened_dim, out_features=9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = self.core(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09571e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShipDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "\n",
    "        #bulk preprocessing\n",
    "        radians = np.deg2rad(self.df['heading'])\n",
    "        self.df['heading_x'] = np.cos(radians)\n",
    "        self.df['heading_y'] = np.sin(radians)\n",
    "\n",
    "        radians = np.deg2rad(self.df['cog'])\n",
    "        self.df['cog_x'] = np.cos(radians)\n",
    "        self.df['cog_y'] = np.sin(radians)\n",
    "\n",
    "        self.df = self.df.fillna(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        #prepare image\n",
    "        img = np.load(row['image_path'])  \n",
    "        img = self.normalize(img)\n",
    "        image_tensor = torch.from_numpy(img)\n",
    "        image_tensor = self.pad_to_600(image_tensor)\n",
    "\n",
    "        #prepare target\n",
    "        target = torch.tensor([\n",
    "            row['sog'],\n",
    "            row['heading_x'],\n",
    "            row['heading_y'],\n",
    "            row['cog_x'],\n",
    "            row['cog_y'],\n",
    "            row['length'],\n",
    "            row['width'],\n",
    "            row['draft'],\n",
    "            row['cargo'],\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        return image_tensor, target\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(arr):\n",
    "        clip_percentiles = (2, 98)\n",
    "        arr = arr.astype(np.float32)\n",
    "        for c in range(arr.shape[0]):\n",
    "            band = arr[c]\n",
    "            band = np.nan_to_num(band, nan=0.0)\n",
    "            vmin, vmax = np.percentile(band, clip_percentiles)\n",
    "            band = np.clip(band, vmin, vmax)\n",
    "            band = (band - vmin) / (vmax - vmin + 1e-5)\n",
    "            arr[c] = band\n",
    "        return arr\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_to_600(tensor):\n",
    "        _, h, w = tensor.shape\n",
    "        pad_h = 600 - h\n",
    "        pad_w = 600 - w\n",
    "\n",
    "        pad_top = pad_h // 2\n",
    "        pad_bottom = pad_h - pad_top\n",
    "        pad_left = pad_w // 2\n",
    "        pad_right = pad_w - pad_left\n",
    "\n",
    "        return F.pad(tensor, (pad_left, pad_right, pad_top, pad_bottom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4feec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, \n",
    "    dataset, \n",
    "    num_epochs=10, \n",
    "    batch_size=32, \n",
    "    lr=1e-4, \n",
    "    val_split=0.1, \n",
    "    device=\"cpu\"\n",
    "                ):\n",
    "\n",
    "    torch.manual_seed(33)\n",
    "\n",
    "    # Split dataset\n",
    "    n = len(dataset)\n",
    "    val_size = int(val_split * n)\n",
    "    train_size = n - val_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for imgs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "        avg_loss = total_loss / train_size\n",
    "        print(f\"Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, targets in val_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                preds = model(imgs)\n",
    "                val_loss += criterion(preds, targets).item() * imgs.size(0)\n",
    "\n",
    "        avg_val = val_loss / val_size\n",
    "        print(f\"Val Loss: {avg_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82ac7b1",
   "metadata": {},
   "source": [
    "# train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ee7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"../../data/ais.db\")\n",
    "df = pd.read_sql_query(\"SELECT * FROM ais\", conn)\n",
    "\n",
    "dataset = ShipDataset(df)\n",
    "model = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fd1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    num_epochs=10,\n",
    "    batch_size=32,\n",
    "    lr=1e-4,\n",
    "    val_split=0.1,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ships-env)",
   "language": "python",
   "name": "ships-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
